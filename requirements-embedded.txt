# Embedded AI Dependencies - LlamaCpp Integration
# This replaces the Ollama dependency with self-contained LLM inference

# Core LlamaCpp integration
llama-cpp-python==0.2.20
# Note: For GPU support, install with specific flags:
# CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python[server]

# System monitoring
psutil==5.9.6
GPUtil==1.4.0

# Existing requirements from the original system
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
sqlalchemy==2.0.23
asyncpg==0.29.0
alembic==1.13.1
redis==5.0.1
bcrypt==4.1.2
python-jose[cryptography]==3.3.0
python-multipart==0.0.6
celery==5.3.4
prometheus-client==0.19.0

# LangChain components (for vector store compatibility)
langchain==0.1.0
langchain-community==0.0.10
langchain-huggingface==0.0.1
faiss-cpu==1.7.4
# For GPU: use faiss-gpu==1.7.4 instead

# HTTP client for HuggingFace integration
requests==2.31.0

# Data processing
numpy==1.24.4
pandas==2.1.4
python-dateutil==2.8.2

# Logging and monitoring
loguru==0.7.2
structlog==23.2.0

# Configuration
python-dotenv==1.0.0
toml==0.10.2

# Development dependencies (optional)
pytest==7.4.3
pytest-asyncio==0.21.1
black==23.11.0
isort==5.12.0
mypy==1.7.1

# Optional: For advanced model quantization
# gguf==0.6.0

# Security
cryptography==41.0.8
passlib[bcrypt]==1.7.4

# WebSocket support
websockets==12.0

# Database drivers
psycopg2-binary==2.9.9

# File handling
pathlib2==2.3.7

# JSON handling
orjson==3.9.10

# Async support
asyncio-mqtt==0.16.1
aiofiles==23.2.1

# Task queue
dramatiq[redis,watch]==1.15.0

# HTTP client async
httpx==0.25.2

# Validation
validators==0.22.0

# Time handling
arrow==1.3.0

# Memory profiling (development)
memory-profiler==0.61.0