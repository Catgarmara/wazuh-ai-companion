# AI-Enhanced Security Query Interface - Deployment Configuration
# This file defines deployment settings for the self-contained security appliance
# with embedded AI processing and no external dependencies

environments:
  development:
    compose_file: "docker-compose.yml"
    profiles: ["monitoring"]
    health_check_timeout: 120
    required_services: ["app", "postgres", "redis"]
    optional_services: ["prometheus", "grafana", "alertmanager"]
    resource_limits:
      app:
        cpu: "1.0"
        memory: "1G"
      postgres:
        cpu: "0.5"
        memory: "512M"
      redis:
        cpu: "0.25"
        memory: "256M"
    
  staging:
    compose_file: "docker-compose.prod.yml"
    profiles: ["monitoring"]
    health_check_timeout: 180
    required_services: ["app", "postgres", "redis", "nginx"]
    optional_services: ["prometheus", "grafana", "alertmanager"]
    resource_limits:
      app:
        cpu: "1.5"
        memory: "2G"
      postgres:
        cpu: "1.0"
        memory: "1G"
      redis:
        cpu: "0.5"
        memory: "512M"
      nginx:
        cpu: "0.25"
        memory: "128M"
    
  production:
    compose_file: "docker-compose.prod.yml"
    profiles: []
    health_check_timeout: 300
    required_services: ["app", "postgres", "redis", "nginx"]
    optional_services: []
    resource_limits:
      app:
        cpu: "2.0"
        memory: "4G"
      postgres:
        cpu: "2.0"
        memory: "4G"
      redis:
        cpu: "1.0"
        memory: "1G"
      nginx:
        cpu: "0.5"
        memory: "256M"

# Health check endpoints for different services
health_checks:
  app: "http://localhost:8000/health"
  app_detailed: "http://localhost:8000/health/detailed"
  metrics: "http://localhost:8000/metrics"
  prometheus: "http://localhost:9090/-/ready"
  grafana: "http://localhost:3000/api/health"
  alertmanager: "http://localhost:9093/-/ready"
  node_exporter: "http://localhost:9100/metrics"
  postgres_exporter: "http://localhost:9187/metrics"
  redis_exporter: "http://localhost:9121/metrics"
  cadvisor: "http://localhost:8080/metrics"

# Kubernetes configuration for embedded AI appliance
kubernetes:
  namespace: "security-ai-appliance"
  ingress:
    enabled: true
    host: "security-ai.example.com"
    tls_enabled: true
  storage:
    postgres_size: "20Gi"
    redis_size: "10Gi"
    embedded_models_size: "200Gi"  # Large storage for local AI models
    vectorstore_data_size: "100Gi"  # Increased for embedded processing
    prometheus_size: "30Gi"
    grafana_size: "10Gi"
  replicas:
    app: 2  # Optimized for embedded processing
    postgres: 1
    redis: 1

# Monitoring configuration
monitoring:
  prometheus:
    retention: "15d"
    scrape_interval: "15s"
    evaluation_interval: "15s"
  grafana:
    admin_user: "admin"
    allow_signup: false
  alertmanager:
    retention: "120h"
    group_wait: "10s"
    group_interval: "10s"
    repeat_interval: "1h"

# Security configuration for embedded appliance
security:
  enable_https: true
  ssl_cert_path: "./nginx/ssl/cert.pem"
  ssl_key_path: "./nginx/ssl/key.pem"
  cors_origins:
    - "http://localhost:3000"
    - "https://security-ai.example.com"
  rate_limiting:
    enabled: true
    requests_per_minute: 60
  embedded_ai_security:
    model_validation: true
    input_sanitization: true
    output_filtering: true

# Backup configuration for embedded appliance
backup:
  enabled: true
  schedule: "0 2 * * *"  # Daily at 2 AM
  retention_days: 30
  s3_bucket: "security-ai-appliance-backups"
  databases:
    - postgres
  volumes:
    - embedded_models  # Local AI models
    - vectorstore_data
    - prometheus_data
    - grafana_data
  embedded_ai:
    backup_models: true
    backup_model_configs: true
    backup_huggingface_cache: true

# Logging configuration
logging:
  level: "INFO"
  format: "json"
  centralized: true
  retention_days: 30
  max_file_size: "100MB"

# Performance tuning for embedded AI appliance
performance:
  postgres:
    max_connections: 200
    shared_buffers: "512MB"
    effective_cache_size: "2GB"
  redis:
    maxmemory: "1GB"
    maxmemory_policy: "allkeys-lru"
  app:
    workers: 4
    worker_connections: 1000
    keepalive_timeout: 65
  embedded_ai:
    max_concurrent_models: 3
    model_memory_limit: "8GB"  # Increased for larger models
    inference_timeout: 60      # Extended for complex queries
    gpu_memory_fraction: 0.8   # If GPU available
    model_cache_size: "16GB"   # Cache for model hot-swapping
    vectorstore_cache_size: "4GB"  # Vector search optimization